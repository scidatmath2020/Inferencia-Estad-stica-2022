{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas de normalidad\n",
    "\n",
    "Frecuentemente nos encontramos frente al problema de reconocer si una muestra proviene de una población gaussiana. \n",
    "\n",
    "Aunque ya hemos visto ajuste de distribuciones, para el caso de gaussianas tenemos más métodos para establecer, con cierta probabilidad, si una muestra proviene realmente de una distribución normal. Estos métodos están basados en pruebas de hipótesis, donde la hipótesis nula tiene la forma \"la población es gaussiana\", en tanto que la alternativa es \"la población no es gaussiana\".\n",
    "\n",
    "De esta manera, a cada una de las pruebas se le asigna un estadístico de contraste con el cual se calcula un $p$-valor, de modo que podemos escribirlas así:\n",
    "\n",
    "$$\\left\\{\\begin{array}{l}H_0:\\mbox{ la población es gaussiana}\\\\H_1:\\mbox{ la población no es gaussiana}\\end{array}\\right.$$\n",
    "\n",
    "con regla de decisión:\n",
    "\n",
    "$p$-valor|Decisión|Significado\n",
    ":--|:--|:--\n",
    "Pequeño|Rechazar $H_0$|Hay buena probabilidad de que **NO es gaussiana**\n",
    "Grande|Rechazar $H_1$|Hay buena probabilidad de que **SÍ es gaussiana**\n",
    "\n",
    "En este capítulo estudiaremos cuatro contrastes que nos ayudan para este fin.\n",
    "\n",
    "***\n",
    "***\n",
    "**Observación.** Es importante mencionar que los contraste que estudiaremos vienen dentro de la paquetería ``fBasics`` de **R**. Las funciones que permiten calcular los estadísticos de contraste y , por lo tanto, los $p$-valores correspondientes, siempre tienen la siguiente sintaxis:\n",
    "\n",
    "* ``nombreTest(muestra)`` para obtener la información general.\n",
    "* ``nombreTest(muestra)@test`` para obtener la información general en forma de lista.\n",
    "* ``nombreTest(muestra)@test$statistic`` para ver el valor del estadístico de contraste.\n",
    "* ``nombreTest(muestra)@test$p.value`` para ver el $p$-valor obtenido.\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las pruebas que realizaremos, tomemos los siguientes datos:\n",
    "\n",
    "``muestra_gaussiana  <- c(2.9267007, 2.5763093, 4.9931801, 0.6564296, 1.4377333, 7.6412183, 2.9204735)``\n",
    "\n",
    "``muestra_t  <- c(-0.577103929579228, -0.0669625949987604, 0.123572935953355, -0.524985500797433, -1.23249669279686, 0.509597230395874, -0.729559305649031, -0.41684441016622, 1.28155478163868, 0.924508782035897, 0.827405247774813, 1.59785194962189, -1.47879497630707, -1.26201626124022, -0.0593983026205043, -0.178873361732746, 0.801185847793428, 0.333473064862654, 1.25186288055626, 2.35949695172828, -0.633493106081742, -1.05713142223298, 0.0212461334293823, 0.466063027431909, 0.0762121526958427, -0.843837287109611, -0.104022595760381, 5.78550093074697, 0.709799846598426, -0.0897824055310009, -0.999402655342385, 0.337761665033848, -0.0306307006025367, 1.47728344947859, -0.176164802725808, 0.690341335235668, -0.292183630229324, -0.844902899428558, -3.49551302890857, 1.43006662844371, 1.24850000914668, -0.180820066444685, -0.573485189819109, 0.349757398842014, -2.09754115696913, -0.352572352149588, -0.509125036161415, 0.712742491824159, 0.519051722042105, -3.00737218678664)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``muestra_gaussiana`` es una muestra de una $N(\\mu=3.5,\\sigma=2)$ y ``muestra_t`` es una $t(3)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de Kolmogorov-Smirnov-Lilliefors (KSL)\n",
    "\n",
    "Recordemos que nuestra $H_0$ es \"la población es gaussiana\". La Prueba KSL consiste en, suponiendo que $H_0$ es verdad, ¿quiénes tendrían que ser la media y la desviación poblacional de la característica? \n",
    "\n",
    "Para ello, se estiman estos parámetros (la media muestral para la media poblacional y la desviación muestral para la desviación poblacional) y se utiliza un cierto estadístico de contraste, llamado *de Lilliefors*, con el cual se calcula el $p$-valor. \n",
    "\n",
    "Sin embargo, tiene un inconveniente: aunque es muy bueno para detectar diferencias \"centrales\" entre la densidad emprírica y una gaussiana, le cuesta trabajo analizar las colas de las densidades. Es decir, si la distribución de la muestra no parece gaussiana \"en el medio\" pero sí en los extremos, esta prueba puede dar como resultado gaussianidad, aún cuando la población no lo sea.\n",
    "\n",
    "En **R** viene dada por ``lillieTest``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de Anderson-Darling (AD)\n",
    "\n",
    "Esta prueba resuelve el inconveniente de KSL. Viene con la función ``adTest``: \n",
    "\n",
    "``adTest(muestra_gaussiana)``\n",
    "\n",
    "``adTest(muestra_t)``\n",
    "\n",
    "Sin embargo, ahora el inconveniente puede venir del tamaño de la muestra: AD trabaja bien para muestras \"medianas\" e incluso pequeñas ($n>7$). Para muestras mayores a 1000, se ha detectado que puede llegar a fallar en muchos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de Shapiro-Wilks (SW)\n",
    "\n",
    "Para muestras de tamaño mayor a 1000, se recomienda utilizar la Prueba de Shapiro Wilks. Viene implementado en **R** con la función ``shapiroTest``. La desventaja que tiene es que admite únicamente muestras de tamaño menor a 5001. Si tu muestra es grande, se recomienda aplicar remuestreo y calcular el promedio de los p-valores obtenidos.\n",
    "\n",
    "Por ejemplo, si ``muestra`` es tu muestra y tiene mas de 5000 elementos, toma unas 30 submuestras de tamaño 5000:\n",
    "\n",
    "``mean(unlist(lapply(1:30,function(x){shapiroTest(sample(muestra,5000))@test$p.value})))``\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de D'Agostino (D)\n",
    "\n",
    "Finalmente, otra prueba muy común es la de D'Agostino. En ella se calcula la curtosis y el sesgo de la muestra y a partir de ella se construye el estadístico de contraste llamado **omnibus** (que es aproximadamente una chi cuadrada con dos grados de libertad si es que la población es gaussiana).\n",
    "\n",
    "En **R** se encuentra con el nombre de ``dagoTest``:\n",
    "\n",
    "``dagoTest(muestra)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "A continuación mostramos una tabla resumen de estos contrastes de normalidad. **Recuerda que para utilizarlos en R requieres la paquetería ``fBasics``**\n",
    "\n",
    "\n",
    "Prueba|Abreviatura|Tamaño de<br> muestra|Desventaja|Estadístico|**R**\n",
    ":--|:--:|:--:|:--|:--:|:--\n",
    "Kolmogorov-Smirnov-Lilliefors|KSL|$5\\le n$|No detecta bien las colas|D|``lillieTest()``\n",
    "Anderson-Darling|AD|$7\\le n\\le1000$|No trabaja bien en muestras grandes|A|``adTest()``\n",
    "Shapiro-Wilks|SW|En **R** admite $\\le5$|Sensible a repeticiones|W|``shapiroTest()``\n",
    "D'Agostino|D|$20\\le n$||Omnibus|``dagoTest()``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
