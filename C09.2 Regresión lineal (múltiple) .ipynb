{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión lineal múltiple\n",
    "\n",
    "En la regresión lineal simple, estudiamos si una variable dependiente $Y$ depende linealmente de una variable independiente o de control $X$.\n",
    "\n",
    "En la práctica, dicha situación rara vez se da, ya que la variable dependiente suele depender de mas un control. Por lo tanto, en esta sección vamos a generalizar todo el estudio que hemos hecho para la regresión lineal simple al caso en que tengamos $k$ variables de control $X_1,X_2,\\cdots,X_k$. Es decir, una variable dependiente $Y$ y $k$ variables de control (si se tienen al menos dos variables dependientes, en la práctica lo que se hace es establecer al menos dos modelos: uno por variable dependiente).\n",
    "\n",
    "Suponemos ahora que nuestro modelo es de la forma $$Y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots+\\varepsilon$$ \n",
    "\n",
    "Los valores $\\beta_0,\\beta_1,...,\\beta_k$ son los llamados parámetros de la regresión y se tienen que estimar a partir de una muestra las variables consideradas: $\\{(x_{i1},x_{i2},...,x_{ik},y_i)\\}_{i=1}^n$.\n",
    "\n",
    "Para que estas estimaciones se pueden realizar hay que suponer que $n>k$ ya que en caso contrario tendríamos un problema subestimado: tendríamos más parámetros que valores en la muestra.\n",
    "\n",
    "Denotamos por $\\boldsymbol{x_i}$ a los valores de las $k$ variables independientes correspondientes al $i$-ésimo elemento de la muestra. Esto es: $\\boldsymbol{x_i}=(x_{i1},x_{i2},\\cdots,x_{ik})$.\n",
    "\n",
    "\n",
    "A partir de la muestra $\\{(\\boldsymbol{x_i},y_i)\\}_{i=1}^n$ vamos a obtener estimaciones $b_0,b_1,\\cdots,b_k$ para los parámetros $\\beta_1,\\beta_2,\\cdots,\\beta_k$.\n",
    "\n",
    "Una vez obtenidas estas estimaciones, podemos definir los valores siguientes:\n",
    "\n",
    "$$\\begin{array}{l}\\widehat{y_i}=b_0+b_1x_{i1}+...+b_kx_{ik},\\\\y_i=b_0+b_1x_{i1}+...+b_kx_{ik}+e_i\\end{array}$$\n",
    "\n",
    "Para simplificar la notación, escribimos los datos de la muestra en forma matricial. En primer lugar, definimos los vectores siguientes:\n",
    "\n",
    "$$\\boldsymbol{y}=\\left(\\begin{array}{c}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{array}\\right),\\,\\boldsymbol{b}=\\left(\\begin{array}{c}b_0\\\\b_1\\\\\\vdots\\\\b_k\\end{array}\\right),\\,\\boldsymbol{\\widehat{y}}=\\left(\\begin{array}{c}\\widehat{y_1}\\\\\\widehat{y_2}\\\\\\vdots\\\\\\widehat{y_n}\\end{array}\\right),\\,\\boldsymbol{e}=\\left(\\begin{array}{c}e_1\\\\e_2\\\\\\vdots\\\\e_n\\end{array}\\right),\\,$$\n",
    "\n",
    "\n",
    "Definimos $\\boldsymbol{\\mathrm{X}}$ a partir de los datos de la muestra de las variables $\\boldsymbol{X}_i$:\n",
    "$$\\boldsymbol{\\mathrm{X}}=\\left(\\begin{array}{ccccc}1&x_{11}&x_{12}&\\cdots&x_{1k}\\\\1&x_{21}&x_{22}&\\cdots&x_{2k}\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\1&x_{n1}&x_{n2}&\\cdots&x_{nk}\\end{array}\\right)$$\n",
    "\n",
    "Por lo tanto $\\boldsymbol{\\widehat{y}}=\\boldsymbol{\\mathrm{X}}\\boldsymbol{b}$ y $\\boldsymbol{y}=\\boldsymbol{\\mathrm{X}\\boldsymbol{b}}+\\boldsymbol{e}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de los parámetros por mínimos cuadrados.\n",
    "\n",
    "Definimos el error cuadrático $SS_E$ como $$SS_E=\\sum_{i=1}^ne_i=\\sum_{i=1}^n(y_i-\\widehat{y_i})^2=\\sum_{i=1}^n(y_i-(b_0+b_1x_{i1}+...+b_kx_{ik}))^2=\\|\\boldsymbol{y}-\\boldsymbol{\\mathrm{X}}\\boldsymbol{b}\\|^2$$\n",
    "\n",
    "Los estimadores $b_0,b_1,\\cdots,b_k$ que buscamos son los que minimicen a $SS_E$.\n",
    "\n",
    "**Teorema.** Los estimadores por el método de los mínimos cuadrados de los parámetros $\\{\\beta_i\\}$ a partir de la muestra $(\\boldsymbol{x_i},y_i)_{i=1}^n$, son los siguientes: $$\\boldsymbol{b}=(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}( \\boldsymbol{\\mathrm{X}}^T\\boldsymbol{y})$$ (ver el pdf [DemostracionMS](https://github.com/scidatmath2020/Inferencia-Estadistica/blob/master/DemostracionMS.pdf) en nuestro repositorio).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal múltiple en R\n",
    "\n",
    "Para calcular los coeficientes de la regresión en R por mínimos cuadrados, de nuevo usamos la función ``lm``:\n",
    "\n",
    "``lm(Y~Variable_1+Variable_2+...+Variable_k,datos)``\n",
    "\n",
    "Si ``r`` es el resultado del modelo (es decir, ``r = lm(Y~Variable_1+Variable_2+...+Variable_k,datos)``), entonces ``summary(r)`` nos da toda la información sobre $p$-valores importante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "**Ejemplo 1.**\n",
    "\n",
    "Un estudio quiere generar un modelo que permita predecir la esperanza de vida media de los habitantes de una ciudad en función de diferentes variables. Se dispone de información sobre: habitantes, analfabetismo, ingresos, esperanza de vida, asesinatos, universitarios, heladas, área y densidad poblacional.\n",
    "\n",
    "La información la encuentras en la tabla [muestra_calidad_vida.csv](https://github.com/scidatmath2020/Inferencia-Estad-stica-2022/blob/main/data/muestra_calidad_vida.csv)\n",
    "\n",
    "El modelo lineal múltiple se calcula con \n",
    "\n",
    "``regresion_original <- lm(esp_vida~habitantes+ingresos+\n",
    "                           analfabetismo+asesinatos+\n",
    "                           universitarios+heladas+\n",
    "                           area+densidad_pobl,data=muestra)``\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coeficiente de determinación.\n",
    "\n",
    "Al igual que hicimos con la regresión lineal simple, vamos a definir el coeficiente de determinación que es una manera de medir lo efectiva que es la regresión.\n",
    "\n",
    "Como antes, este se define a través de la **identidad de suma de cuadrados**:\n",
    "\n",
    "$$\\underbrace{\\sum_{i=1}^n(Y_i-\\overline{Y})^2}_{SS_{Total}}=\\underbrace{\\sum_{i=1}^n(\\hat{Y_i}-\\overline{Y})^2}_{SS_{Regresión}}+\\underbrace{\\sum_{i=1}^n(Y_i-\\hat{Y_i})^2}_{SS_{Error}}$$\n",
    "\n",
    "Se define el **coeficiente de determinación $R^2$** en la regresión por el método de los mínimos cuadrados como $$R^2=\\frac{SS_{Regresión}}{SS_{Total}}$$\n",
    "\n",
    "$R^2$ es una cantidad entre 0 y 1. Cuanto más próximo a 1 esté dicho coeficiente, más precisa será la recta de regresión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de modelos: coeficiente de determinación ajustado \n",
    "\n",
    "El coeficiente de determinación definido anteriormente aumenta si aumentamos el número de variables independientes $k$, incluso si estas aportan información redundante o poca información. Por ejemplo, si añadimos variables que son linealmente dependientes de las demás.\n",
    "\n",
    "Para evitar este problema, o para penalizar el aumento de variables independientes se usa en su lugar el **coeficiente de regresión ajustado:** $$R^2_{adj}=\\frac{MS_T-MS_E}{MS_T}$$ donde $MS_T=\\frac{SS_T}{n-1}$ y $MS_E=\\frac{SS_E}{n-k-1}$. La relación entre ambos coeficientes de determinación es $$R^2_{adj}=1-(1-R^2)\\frac{n-1}{n-k-1}$$\n",
    "\n",
    "En general, $0\\le R^2_{adj}<R^2\\le1$, por lo el coeficiente de determinación ajustado es más difícil obtener un valor cercano a 1.\n",
    "\n",
    "### Coeficientes de determinación en R\n",
    "\n",
    "En **R**, con ``summary(r)`` se obtienen ambos coeficientes de determinación, donde ``r`` es el resultado de la regresión. En caso de verlos directamente, utilizamos\n",
    "\n",
    "* ``summary(r)$r.squared`` para $R^2$\n",
    "* ``summary(r)$adj.r.squared`` para el $R^2$ ajustado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "**Ejemplo 1. Coeficientes de determinación**\n",
    "\n",
    "``summary(regresion_original)``\n",
    "\n",
    "``summary(regresion_original)$r.squared``\n",
    "\n",
    "``summary(regresion_original)$adj.r.squared``\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de modelos: AIC y BIC.\n",
    "\n",
    "Cuando comparamos dos modelos lineales múltiples para ver cuál es más adecuado que el otros, el coeficiente de regresión ajustado es uno de los métodos para tomar la decisión: siempre buscaremos aquel con mayor $R^2$ ajustado.\n",
    "\n",
    "Existen más métodos como son el AIC o el BIC (criterio de Akaike y criterio bayesiano). En ambos, buscamos el de menor información.\n",
    "\n",
    "### Índice de Akaike\n",
    "\n",
    "El AIC cuantifica cuánta información de $Y$ se pierde con el modelo y cuántas variables usamos. Concretamente, se define como $$AIC=n\\ln\\left(\\frac{SS_E}{n}\\right)+2k$$ y el modelo con menor AIC es el más adecuado.\n",
    "\n",
    "En R, para calcularlo usamos ``AIC(r)`` donde ``r`` es el modelo lineal obtenido..\n",
    "\n",
    "### Índice bayesiano\n",
    "\n",
    "Análogo al método de Akaike, el BIC cuantifica cuánta información de $Y$ se pierde con el modelo y cuántas variables usamos. Concretamente, se define como $$BIC=n\\ln\\left(\\frac{SS_E}{n}\\right)+k\\ln(n)$$ y el modelo con menor BIC es el más adecuado.\n",
    "\n",
    "En R, para calcularlo usamos ``BIC(r)`` donde ``r`` es el modelo lineal obtenido.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de modelos en R\n",
    "\n",
    "Supongamos que tienes $k$ variables explicativas. Entonces puedes formar $2^k-1$ modelos lineales diferentes con todas ellas. Para hallar el mejor, **R** tiene la función ``step``, la cual encuentra el mejor mediante el criterio de Akaike.\n",
    "\n",
    "``step(r)`` donde ``r`` es el modelo.\n",
    "\n",
    "***\n",
    "***\n",
    "**Ejemplo 1. Comparación de modelos**\n",
    "\n",
    "Haciendo ``step(regresion_original)``, observamos que el modelo con menor información de Akaike es \n",
    "\n",
    "$$Y=\\beta_0+\\beta_1\\cdot\\mbox{Ingresos}+\\beta_2\\cdot\\mbox{Asesinatos}+\\beta_3\\cdot\\mbox{Universitarios}+\\beta_4\\cdot\\mbox{Área}$$\n",
    "\n",
    "Por lo tanto tomamos \n",
    "\n",
    "``\n",
    "regresion_simplificada <- lm(esp_vida~ingresos+asesinatos +\n",
    "                               universitarios+area,data=muestra)\n",
    "``\n",
    "\n",
    "A su vez, con ``summary(regresion_simplificada)`` podemos observar el $R^2$\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervalos de confianza para los coeficientes\n",
    "\n",
    "Supongamos ahora que los residuos siguen una distribución gaussiana con media cero; además todos tienen la misma desviación y sus correlaciones son cero.\n",
    "\n",
    "En estas condiciones\n",
    "\n",
    "* Un intervalo de confianza del $100(1-\\alpha)\\%$ de confianza para $\\beta_i$ es $$\\left(b_i-t_{n-k-1,1-\\frac{\\alpha}{2}}\\sqrt{\\sigma_E^2(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}_{ii}},b_i+t_{n-k-1,1-\\frac{\\alpha}{2}}\\sqrt{\\sigma_E^2(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}_{ii}}\\right)$$\n",
    "\n",
    "### Intervalos de confianza para los coeficientes en R\n",
    "\n",
    "En R, los intervalos de confianza anteriores se pueden hallar con ``confint(r)`` donde ``r`` es el modelo lineal obtenido. \n",
    "\n",
    "***\n",
    "***\n",
    "**Ejemplo 1. Intervalos de confianza para los coeficientes**\n",
    "\n",
    "Recordemos que ya hemos disminuido el número de variables explicativas y el modelo lo tenemos en ``regresion_simplificada``. Los intervalos de confianza simplemente son ``confint(regresion_simplificada)``.\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significancia de los coeficientes\n",
    "\n",
    "Nos interesa ahora saber cuáles de los coeficientes $\\beta_1,\\beta_2,...,\\beta_k$ son significativamente diferentes de cero. \n",
    "\n",
    "Podemos analizar esta situación en dos pasos: \n",
    "\n",
    "* Por una parte, ¿existe alguno diferente de cero? Es decir, ¿nuestra regresión vale la pena? Para responder esto, bajo las hipótesis sobre los residuos expresadas anteriormente, se plantea la prueba\n",
    "\n",
    "$$\\left\\{\\begin{array}{l}H_0:\\beta_1=\\beta_2=...=\\beta_k=0\\\\H_1:\\mbox{ Existe }i\\mbox{ con }\\beta_i\\neq0\\end{array}\\right.$$\n",
    "\n",
    "El estadístico de contraste de esta prueba es una cierta $F$. Si el $p$-valor es pequeño, concluimos que la regresión vale la pena\n",
    "\n",
    "* En caso de que la respuesta a la pregunta anterior sea positiva, ¿cuáles coeficientes son significativos?\n",
    "\n",
    "Para esto, se plantean las $k$ pruebas\n",
    "\n",
    "$$\\left\\{\\begin{array}{l}H_0:\\beta_i=0\\\\H_1:\\beta_i\\neq0\\end{array}\\right.$$\n",
    "\n",
    "El estadístico de contraste de esta prueba es una cierta $t$. Si el $p$-valor es pequeño, concluimos que el coeficiente $\\beta_i$ es siginificativamente diferente de 0 (por lo tanto los efectos de la característica $X_i$ son importantes).\n",
    "\n",
    "### Significancia de los coeficientes en R\n",
    "\n",
    "En **R**, todos estos $p$-valores vienen dados por la función ``summary(r)`` donde ``r`` es el modelo de regresión lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio de predicciones.\n",
    "\n",
    "Fijados valores concretos de $(x_1,x_2,...,x_n)$, digamos $(x_{01},x_{02},...,x_{0k})$, podemos considerar dos parámetros más a estudiar: el valor medio de la variable aleatoria $Y|_{x_{01},x_{02},...,x_{0k}}$ y el valor estimado $y_0=b_0+b_1x_{10}+...+b_kx_{k0}$ por la regresión.\n",
    "\n",
    "Así, los intervalos de confianza al $100(1-\\alpha)\\%$ de confianza para ambos parámetros vienen dados por $$\\left(\\mu|_{\\boldsymbol{x_0}}-t_{n-k-1,1-\\frac{\\alpha}{2}}S\\sqrt{\\boldsymbol{x_0^\\prime}(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}\\boldsymbol{x_0}^{\\prime T}},\\mu|_{\\boldsymbol{x_0}}+t_{n-k-1,1-\\frac{\\alpha}{2}}S\\sqrt{\\boldsymbol{x_0^\\prime}(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}\\boldsymbol{x_0}^{\\prime T}}\\right)$$ y \n",
    " \n",
    "$$\\left(\\widehat{y_0}-t_{n-k-1,1-\\frac{\\alpha}{2}}S\\sqrt{1+\\boldsymbol{x_0^\\prime}(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}\\boldsymbol{x_0}^{\\prime T}},\\widehat{y_0}+t_{n-k-1,1-\\frac{\\alpha}{2}}S\\sqrt{1+\\boldsymbol{x_0^\\prime}(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}\\boldsymbol{x_0}^{\\prime T}}\\right)$$\n",
    "\n",
    "### Estudio de predicciones en R\n",
    "\n",
    "En **R**, para hallar estos intervalos de confianza hacemos lo siguiente \n",
    "\n",
    "``newdata = data.frame(x1=x10,x2=x20,...,xk=xk0)``\n",
    "\n",
    "``predict.lm(r,newdata,interval = \"¿A QUIÉN VAS A ESTIMAR?\",level = nivel.confianza)``\n",
    "\n",
    "donde ``r`` es el modelo de regresión lineal obtenido; el parámetro interval es ``confidence`` si se desea el intervalo de confianza para $\\mu|_{\\boldsymbol{x_0}}$, y es igual a ``prediction`` si se desea para el parámetro $y_0$.\n",
    "\n",
    "***\n",
    "***\n",
    "**Ejemplo 1. Estudio de predicciones en R**\n",
    "\n",
    "Se sabe que Alabama tiene un ingreso de 3624, asesinatos de 15.1, universitarios de 41.3 y área de 50708, en tanto que en Minnesota se tiene tiene un ingreso de 4675, asesinatos de 2.3, universitarios de 57.6 y área de 79289.\n",
    "\n",
    "Utilizando nuestro modelo, hacemos:\n",
    "\n",
    "``newdata = data.frame(ingresos = c(3624,4675), asesinatos = c(15.1,2.3), universitarios = c(41.3,57.6), area =c(50708,79289))``\n",
    "\n",
    "``predict.lm(regresion_simplificada,newdata,interval = \"confidence\", level = 0.95)``\n",
    "\n",
    "``predict.lm(regresion_simplificada,newdata,interval = \"prediction\", level = 0.95)``\n",
    "\n",
    "Cabe mencionar que de hecho, el valor de la esperanza de vida en Alabama es de 69.05 años y el de Minnesota es de 72.96\n",
    "\n",
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
