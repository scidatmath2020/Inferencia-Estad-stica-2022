{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnósticos de regresión.\n",
    "\n",
    "Para que el modelo de regresión lineal sea fiable, en las conclusiones derivadas de las estimaciones de intervalos e inferencias (intervalos de confianza, contrastes de hipótesis,...) que realizamos a partir de dicho modelo se tienen que verificar cietas hipótesis.\n",
    "\n",
    "Las tareas que realizan dichas verificacones se denominan **diagnósticos de regresión**.\n",
    "\n",
    "Estos se clasifican en tres categorías:\n",
    "\n",
    "* Errores: los errores deben seguir una $N(0,\\sigma)$ con la misma varianza y ser incorrelacionados.\n",
    "\n",
    "\n",
    "* Modelo: los puntos se tienen que ajustar a la estructura lineal considerada.\n",
    "\n",
    "\n",
    "* Observaciones anómalas: a veces unas cuántas observaciones no se ajustan al modelo y hay que detectarlas.\n",
    "\n",
    "\n",
    "En general hay dos métodos: gráficos y numéricos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalidad de los residuos.\n",
    "\n",
    "Para detectar la normalidad de los residuos, podemos aplicar todos los tests de normalidad vistos en el tema de Bondad de Ajuste:\n",
    "\n",
    "* Test Kolmogorov-Smirnov: tomar $\\mu=0$ y para estimar $\\sigma$ usamos la desviación muestral.\n",
    "\n",
    "\n",
    "* Test Kolmogorov-Smirnov-Lilliefors: cuando la muestra es pequeña.\n",
    "\n",
    "\n",
    "* Test Anderson-Darling.\n",
    "\n",
    "\n",
    "* Test de Shapiro-Wilks.\n",
    "\n",
    "\n",
    "* Test D'Agostino-Pearson.\n",
    "\n",
    "\n",
    "Es aconsejable también realizr las pruebas gráficas de normalidad (como los QQ-plots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homocedasticidad.\n",
    "\n",
    "Uno de los problemas que puede sufrir nuestro modelo es que la varianza de los residuos no sea constante. \n",
    "\n",
    "### Test de Breusch-Pagan.\n",
    "\n",
    "Cuando no es posible aplicar el Test de White debido a la **Observación importante** anterior, en su lugar podemos aplicar el Test de Breuch-Pagan. Su aplicación es bastante parecida al de White, pero evita los términos de segundo orden en la regresión auxiliar:\n",
    "\n",
    "* Obtener los residuos $\\{e_i\\}_{i=1}^n$ de la regresión lineal inicial.\n",
    "\n",
    "\n",
    "* Calcular el coeficiente de determinación $R^2$ de la regresión lineal de los $e_i^2$ respecto de las variables iniciales. Es decir, calcular el coeficiente de determinación del modelo siguiente: $$\\mu_{E^2|_{x_1,...,x_k}}=\\beta_0+\\beta_1x_1+\\cdots+\\beta_kx_k$$\n",
    "\n",
    "\n",
    "* Calcular el estadístico $X_0=nR^2$ el cual, suponiendo homocedasticidad, sigue una $\\chi^2_k$.\n",
    "\n",
    "\n",
    "* Calculamos el p-valor $P(\\chi^2_k\\ge X_0)$ con el significado usual.\n",
    "\n",
    "\n",
    "Para realizarlo en R, simplemente usamos ``bptest(lm1)`` donde lm1 es el objeto de R donde hemos guardado la información de la regresión original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlación de los residuos: Test de Durbin-Watson.\n",
    "\n",
    "Otra de las hipótesis que se deben verificar para que el análisis de regresión sea correcto es la incorrelación de los residuos.\n",
    "\n",
    "La autocorrelación de los residuos puede ser de dos tipos:\n",
    "\n",
    "* Positiva: un valor positivo (negativo) de un error genera una cadena de residuos positivos (negativos)\n",
    "\n",
    "\n",
    "* Negativa: los residuos alternan signo.\n",
    "\n",
    "\n",
    "Para comprobar que los residuos no presentan correlación, se puede aplicar el Test de Durbin-Watson.\n",
    "\n",
    "Sean $\\{e_i\\}_{i=1}^n$ los residuos de la regresión. Tomemos $E_i$ y $E_{i-1}$ como las variables aleatorias de error y consideremos la recta de regresión de $E_i$ respecto de $E_{i-1}$. Esto es, consideremos el modelo $E_i=\\beta_0+\\beta_1E_{i-1}$.\n",
    "\n",
    "Se plantea el contraste $$\\left\\{\\begin{array}{l}\\mathcal{H}_0:\\beta_1=0\\\\\\mathcal{H}_1:\\beta_1\\neq0\\end{array}\\right.$$ con el estadístico de contraste $$d=\\frac{\\sum_{i=2}^n(e_i-e_{i-1})^2}{\\sum_{i=1}^ne_i^2}$$\n",
    "\n",
    "El valor de este estadístico es aproximadamente $2(1-b_1)$ donde $b_1$ es una estimación de $\\beta_1$. Si $\\mathcal{H}_0$ es cierta, su distribución es la de una cierta combinación lineal de $\\chi^2$.\n",
    "\n",
    "Este test está implementado en la función ``dwtest`` del paquete **lmtest**. Su sintaxis es\n",
    "\n",
    "``dwtest(lm1,alternative=...)``\n",
    "\n",
    "donde \n",
    "\n",
    "**lm1**: es la regresión original.\n",
    "\n",
    "**alternative** sirve para indicar si se testea autocorrelación positiva o negativa (\"greater\" o \"less\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditividad y linealidad\n",
    "\n",
    "Cuando se plantea un modelo lineal, se suponen implícitamente las condiciones siguientes:\n",
    "\n",
    "* Aditividad: para cada variable independiente $X_i$, la variación de $\\mu_{Y|_{x_1,...,x_k}}$ asociada con un aumento en $X_i$ (manteniendo las otras variables constantes) es la misma sean cuales sean los valores de las otras variables independientes. Es decir, \n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\mu_{Y|_{x_1,...,x_i+h,...x_k}}-\\mu_{Y|_{x_1,...,x_i,...x_k}}&=&\\beta_0+\\beta_1x_1+...+\\beta_i(x_i+h)+...+\\beta_kx_k-(\\beta_0+\\beta_1x_1+...+\\beta_ix_i+...+\\beta_kx_k)\\\\\n",
    "&=&\\beta_ih\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Observemos que la variación en $\\mu$ es independiente del resto de las variables. Por otro lado:\n",
    "\n",
    "* Linealidad: para cada variable independiente $X_i$, la variación de $\\mu_{Y|_{x_1,...,x_k}}$ asociada con un aumento en $X_i$ (manteniendo las otras variables constantes) es la misma sea cual sea el valor de $X_i$ (es decir, en la expresión de arriba no aparece $X_i$). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aditividad.\n",
    "\n",
    "Podemos comprobar la aditividad con el Test de Tukey usando los llamados gráficos de residuos parciales para la linealidad. La idea principal es verificar que no haya interacción entre las variables independienes y así, cada una tendrá un efecto aditivo en el modelo.\n",
    "\n",
    "Si existe la interacción, algunos de los términos cuadráticos tendrán peso en el modelo. Esta es la base del Test de Tukey.\n",
    "\n",
    "Para aplicarlo, se siguen los siguientes pasos:\n",
    "\n",
    "* Se obtienen los valores ajustados $\\{\\widehat{y_i}\\}$ por la regresión lineal inicial.\n",
    "\n",
    "\n",
    "* Se lleva a cabo una segunda regresión lineal incluyendo como nueva variable independiente los $\\widehat{y_i}^2$. Sea $\\beta$ el coeficiente de esta nueva variable.\n",
    "\n",
    "\n",
    "* Se testea si la variable $\\widehat{y}^2$ es significativa en la segunda regresión. Es decir, se realiza el contraste $$\\left\\{\\begin{array}{l}\\mathcal{H}_0:\\beta=0\\\\\\mathcal{H}_1:\\beta\\neq0\\end{array}\\right..$$\n",
    "\n",
    "Si no podemos descartar $\\mathcal{H}_0$, la variable de los $\\widehat{y_i}^2$ no es significativa y el modelo es aditivo.\n",
    "\n",
    "En R, para realizar el Test de Tukey hay que usar la función ``residualPlots`` del paquete **car**:\n",
    "\n",
    "``residualPlots(lm1)``\n",
    "\n",
    "donde \n",
    "\n",
    "**lm1** es el objeto de R donde hemos guardado la información de la regresión original\n",
    "\n",
    "**plot** es un parámetro lógico. Si vale TRUE, nos dibuja los gráficos de los residuos frente a las variables regresoras y frente a los valores estimados junto con una curva de color azul indicando su tendencia; si vale FALSE simplemente da el valor del estadístico de Tukey y su p-valor.\n",
    "\n",
    "\n",
    "Si optamos por ver los gráficos, entonces para concluir satisfactoriamente todos ellos deben parecer cielos estrellados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linealidad.\n",
    "\n",
    "Los gráficos de residuos parciales son una herramienta útil para detectar la no linealidad en una regresión. Se definen los residuos parciales $e_{ij}$ para la variable independiente $X_j$ como $$e_{ij}=e_i+b_jx_{ij},$$ donde $e_i$ es el residuo $i$-ésimo de la regresión lineal; $b_j$ es el coeficiente de $X_j$ en la regresión original y $x_{ij}$ es la observación $j$-ésima del individuo $i$-ésimo.\n",
    "\n",
    "Los residuos parciales se dibujan contra los valores de $x_j$ y se hace su recta de regresión.\n",
    "\n",
    "Si esta no se ajusta a la curva dada por una regresión no paramétrica suave (las variables independientes no están predeterminadas y se construyen con los datos), el modelo no es lineal.\n",
    "\n",
    "La función de $R$ para representar estos gráficos es ``crPlots`` del paquete **car**: ``crPlots(lm1)``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observaciones anómalas.\n",
    "\n",
    "Las observaciones anómalas pueden provocar que se malinterpreten patrones en el conjunto de datos. Además, puntos aislados pueden tener una gran influencia en el modelo de regresión, dando resultados completamente diferentes. Por ejemplo, pueden provocar que nuestro modelo no capture características importantes de los datos. \n",
    "\n",
    "Por ello, es importante detectarlas.\n",
    "\n",
    "Existen tres tipos de observaciones anómalas:\n",
    "\n",
    "* **Outliers de regresión** son observaciones que tienen un valor anómalo de la variable $Y$, condicionado a los valores de sus variables independientes $X_i$. Tendrán un residuo muy alto pero no pueden afectar demasiado a los coeficientes de la regresión.\n",
    "\n",
    "\n",
    "* **Leverages.** son observaciones con un valor anómalo de las variables $X_i$. No tienen por qué afectar los coeficientes de la regresión.\n",
    "\n",
    "\n",
    "* **Observaciones influyentes** son aquellas que tienen un leverage alto; son outliers de regresión y afectan fuertemente a la regresión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverages\n",
    "\n",
    "Para hallar las observaciones que son leverages, necesitamos la matriz Hat: $H=X(X^TX)^{-1}X^T$. Como $\\widehat{y}=Hy$, tenemos que $\\widehat{y_j}=h_{1j}y_1+h_{2j}y_2+...+h_{nj}y_n=\\sum_{i=1}^nh_{ij}x_i$ para $j=1,...,n$.\n",
    "\n",
    "Si $h_{ij}$ es grande, la observación $i$-ésima tiene un impacto sustancial en el valor predicho $j$-ésimo.\n",
    "\n",
    "Se define el leverage de la observación $i$ como su valor hat: $h_i=\\sum_{j=1}^nh_{ij}^2$.\n",
    "hat\n",
    "La regla de decisión que nos dirá si una observación tiene leverage grande (y por lo tanto debe ser considerada con cuidado) es cuando su valor hat es mayor que $2(k+1)/n$.\n",
    "\n",
    "La función ``hatvalues`` de R calcula los valores hat basándose en la regla anterior: ``which(hatvalues(lm1)>2(k+1)/n)``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Para detectar outliers, en R se usa la ``función outlierTest`` del paquete **car**:  ``outlierTest(lm1)``. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones influyentes.\n",
    "\n",
    "El método que usaremos es la llamada Distanca de Cook. Las observaciones influyentes son aquellas cuya distancia de Cook es mayor a $4/(n-k-1)$\n",
    "\n",
    "En R, la distancia de Cook se calcula con ``cooks.distance`` del paquete **car**: ``which(cooks.distance(lm1)>4/(n-k-1))``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamiento de las observaciones anómalas.\n",
    "\n",
    "El tratamiento suele ser complejo. \n",
    "\n",
    "Nos debemos preguntar si se deben a errores en la entrada o recogida de los datos y si este es el caso, eliminarlas.\n",
    "\n",
    "Pero también pueden explicar que no se ha considerado alguna variable independiente que afecta al conjunto de observaciones anómalas.\n",
    "\n",
    "Las más peligrosas son las influyentes.\n",
    "\n",
    "En el supuesto de que se determine que se pueden eliminar, se tienen que eliminar de una a una actualizando el modelo cada vez. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
