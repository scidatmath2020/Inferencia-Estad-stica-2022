{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnósticos de regresión.\n",
    "\n",
    "Para que el modelo de regresión lineal sea fiable, en las conclusiones derivadas de las estimaciones de intervalos e inferencias (intervalos de confianza, contrastes de hipótesis,...) que realizamos a partir de dicho modelo se tienen que verificar cietas hipótesis.\n",
    "\n",
    "Las tareas que realizan dichas verificacones se denominan **diagnósticos de regresión**.\n",
    "\n",
    "Estos se clasifican en tres categorías:\n",
    "\n",
    "* Errores: los errores deben seguir una $N(0,\\sigma)$ con la misma varianza y ser incorrelacionados.\n",
    "\n",
    "\n",
    "* Modelo: los puntos se tienen que ajustar a la estructura lineal considerada.\n",
    "\n",
    "\n",
    "* Observaciones anómalas: a veces unas cuántas observaciones no se ajustan al modelo y hay que detectarlas.\n",
    "\n",
    "\n",
    "En general hay dos métodos: gráficos y numéricos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalidad de los residuos.\n",
    "\n",
    "Para detectar la normalidad de los residuos, podemos aplicar todos los tests de normalidad vistos en el tema de Bondad de Ajuste:\n",
    "\n",
    "* Test Kolmogorov-Smirnov-Lilliefors.\n",
    "\n",
    "\n",
    "* Test Anderson-Darling.\n",
    "\n",
    "\n",
    "* Test de Shapiro-Wilks.\n",
    "\n",
    "\n",
    "* Test D'Agostino-Pearson.\n",
    "\n",
    "### Normalidad de los residuos en R\n",
    "\n",
    "En **R**, si ``r`` es el modelo, entonces ``r$residuals`` es el vector de errores. Por lo tanto es a este a vector a quien le tenemos que hacer la prueba de gaussianidad. Recuerda que los test de gaussianidad requieren la paquetería ``fBasics`` y se sigue la regla de decisión\n",
    "\n",
    "$p$-valor|Decisión|Significado\n",
    ":--|:--|:--\n",
    "Pequeño|Rechazar $H_0$|Hay buena probabilidad de que **NO es gaussiana**\n",
    "Grande|Rechazar $H_1$|Hay buena probabilidad de que **SÍ es gaussiana**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homocedasticidad.\n",
    "\n",
    "Para verificar la homocedasticidad de los residuos, podemos aplicar el **Test de Breuch-Pagan**. Su aplicación es bastante sencilla:\n",
    "\n",
    "* Obtener los residuos $\\{e_i\\}_{i=1}^n$ de la regresión lineal inicial.\n",
    "\n",
    "\n",
    "* Calcular el coeficiente de determinación $R^2$ de la regresión lineal de los $e_i^2$ respecto de las variables iniciales. Es decir, calcular el coeficiente de determinación del modelo siguiente: $$E^2=\\gamma_0+\\gamma_1X_1+\\cdots+\\gamma_kX_k$$\n",
    "\n",
    "\n",
    "* Calcular el estadístico $X_0=nR^2$ el cual, suponiendo homocedasticidad, sigue una $\\chi^2_k$.\n",
    "\n",
    "\n",
    "* Calculamos el p-valor $P(\\chi^2_k\\ge X_0)$ con el significado usual.\n",
    "\n",
    "\n",
    "La regla de decisión es \n",
    "\n",
    "$p$-valor|Decisión|Significado\n",
    ":--|:--|:--\n",
    "Pequeño|Rechazar $H_0$|Hay buena probabilidad de que los residuos **NO sean homocedásticos**\n",
    "Grande|Rechazar $H_1$|Hay buena probabilidad de que los residuos **SÍ sean homocedásticos**\n",
    "\n",
    "También se puede hacer una prueba gráfica: cuando se grafica la nube de puntos de los valores ajustados (es decir, las $\\hat{y_i}$) vs los residuos, deberemos observar un **cielo estrellado** centrado en la recta $y=0$.\n",
    "\n",
    "### Homocedasticidad en R\n",
    "\n",
    "Para realizarlo en R, simplemente usamos ``bptest(r)`` de la paquetería ``lmtest``, donde ``r`` es el objeto de R donde hemos guardado la información de la regresión original.\n",
    "\n",
    "Y para realizar la gráfica hacemos\n",
    "\n",
    "``ggplot(data = datos, aes(r$fitted.values, r$residuals)) +\n",
    "    geom_point() +\n",
    "    geom_hline(yintercept = 0) +\n",
    "    theme_bw()``\n",
    "\n",
    "donde ``datos`` es tu tabla original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelación de los residuos: Test de Durbin-Watson.\n",
    "\n",
    "Otra de las hipótesis que se deben verificar para que el análisis de regresión sea correcto es la incorrelación de los residuos.\n",
    "\n",
    "La autocorrelación de los residuos puede ser de dos tipos:\n",
    "\n",
    "* Positiva: un valor positivo (negativo) de un error genera una cadena de residuos positivos (negativos)\n",
    "\n",
    "\n",
    "* Negativa: los residuos alternan signo.\n",
    "\n",
    "\n",
    "Para comprobar que los residuos no presentan correlación, se puede aplicar el Test de Durbin-Watson.\n",
    "\n",
    "Sean $\\{e_i\\}_{i=1}^n$ los residuos de la regresión. Tomemos $E_i$ y $E_{i-1}$ como las variables aleatorias de error y consideremos la recta de regresión de $E_i$ respecto de $E_{i-1}$. Esto es, consideremos el modelo $E_i=\\beta_0+\\beta_1E_{i-1}$.\n",
    "\n",
    "Se plantea el contraste $$\\left\\{\\begin{array}{l}\\mathcal{H}_0:\\beta_1=0\\\\\\mathcal{H}_1:\\beta_1\\neq0\\end{array}\\right.$$ con el estadístico de contraste $$d=\\frac{\\sum_{i=2}^n(e_i-e_{i-1})^2}{\\sum_{i=1}^ne_i^2}$$\n",
    "\n",
    "El valor de este estadístico es aproximadamente $2(1-b_1)$ donde $b_1$ es una estimación de $\\beta_1$. Si $\\mathcal{H}_0$ es cierta, su distribución es la de una cierta combinación lineal de $\\chi^2$.\n",
    "\n",
    "La regla de decisión es\n",
    "\n",
    "$p$-valor|Decisión|Significado\n",
    ":--|:--|:--\n",
    "Pequeño|Rechazar $H_0$|Hay buena probabilidad de que los residuos **SÍ tengan autocorrelación**\n",
    "Grande|Rechazar $H_1$|Hay buena probabilidad de que los residuos **NO tengan autocorrelación**\n",
    "\n",
    "### Autocorrelación de los residuos en R\n",
    "\n",
    "Este test está implementado en la función ``dwt`` del paquete ``car``. Su sintaxis es ``dwt(r,alternative=\"two.sided\")``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditividad y linealidad\n",
    "\n",
    "Sea $i$ fija y consideremos y perturbemos la variable $X_i$, digamos con $x_i+h$. Entonces\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\boldsymbol{Y|_{x_1,...,x_i+h,...x_k}-Y|_{x_1,...,x_i,...x_k}}&\\boldsymbol{=}&\\boldsymbol{\\beta_0+\\beta_1x_1+...+\\beta_i(x_i+h)+...+\\beta_kx_k-(\\beta_0+\\beta_1x_1+...+\\beta_ix_i+...+\\beta_kx_k)}\\\\\n",
    "&\\boldsymbol{=}&\\boldsymbol{\\beta_ih}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "Cuando se plantea un modelo lineal, se suponen implícitamente las condiciones siguientes:\n",
    "\n",
    "* Aditividad: la variación en la expresión en **negrita** es independiente de las variables que no se perturbaron.\n",
    "\n",
    "* Linealidad: la variación en la expresión en **negrita** es la misma independientemente de qué variable se perturbó. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditividad.\n",
    "\n",
    "Podemos comprobar la aditividad con el Test de Tukey usando los llamados gráficos de residuos parciales. La idea principal es verificar que no haya interacción entre las variables independienes y así, cada una tendrá un efecto aditivo en el modelo.\n",
    "\n",
    "Si existe la interacción, algunos de los términos cuadráticos tendrán peso en el modelo. Esta es la base del Test de Tukey.\n",
    "\n",
    "Para aplicarlo, se siguen los siguientes pasos:\n",
    "\n",
    "* Se obtienen los valores ajustados $\\{\\widehat{y_i}\\}$ por la regresión lineal inicial.\n",
    "\n",
    "\n",
    "* Se lleva a cabo una segunda regresión lineal incluyendo como nueva variable independiente los $\\widehat{y_i}^2$. Sea $\\beta$ el coeficiente de esta nueva variable.\n",
    "\n",
    "\n",
    "* Se testea si la variable $\\widehat{y}^2$ es significativa en la segunda regresión. Es decir, se realiza el contraste $$\\left\\{\\begin{array}{l}\\mathcal{H}_0:\\beta=0\\\\\\mathcal{H}_1:\\beta\\neq0\\end{array}\\right..$$\n",
    "\n",
    "\n",
    "Si no podemos descartar $\\mathcal{H}_0$, la variable de los $\\widehat{y_i}^2$ no es significativa y el modelo es aditivo.\n",
    "\n",
    "La regla de decisión es\n",
    "\n",
    "$p$-valor|Decisión|Significado\n",
    ":--|:--|:--\n",
    "Pequeño|Rechazar $H_0$|Hay buena probabilidad de que el modelo **NO es aditivo**\n",
    "Grande|Rechazar $H_1$|Hay buena probabilidad de que el modelo **SÍ es aditivo**\n",
    "\n",
    "### Aditividad en R\n",
    "\n",
    "En R, para realizar el Test de Tukey hay que usar la función ``residualPlots`` del paquete ``car``:\n",
    "\n",
    "``residualPlots(r,plot=...)``\n",
    "\n",
    "donde \n",
    "\n",
    "``r`` es el objeto de R donde hemos guardado la información de la regresión original\n",
    "\n",
    "``plot`` es un parámetro lógico. Si vale TRUE, nos dibuja los gráficos de los residuos frente a las variables regresoras y frente a los valores estimados junto con una curva de color azul indicando su tendencia; si vale FALSE simplemente da el valor del estadístico de Tukey y su $p$-valor.\n",
    "\n",
    "\n",
    "Si optamos por ver los gráficos, entonces para concluir satisfactoriamente todos ellos deben parecer cielos estrellados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linealidad.\n",
    "\n",
    "Los gráficos de residuos parciales son una herramienta útil para detectar la no linealidad en una regresión. Se definen los residuos parciales $e_{ij}$ para la variable independiente $X_j$ como $$e_{ij}=e_i+b_jx_{ij},$$ \n",
    "\n",
    "donde $e_i$ es el residuo $i$-ésimo de la regresión lineal; $b_j$ es el coeficiente de $X_j$ en la regresión original y $x_{ij}$ es la observación $j$-ésima del individuo $i$-ésimo.\n",
    "\n",
    "Los residuos parciales se dibujan contra los valores de $x_j$ y se hace su recta de regresión.\n",
    "\n",
    "Si esta no se ajusta a la curva dada por una regresión no paramétrica suave (las variables independientes no están predeterminadas y se construyen con los datos), el modelo no es lineal.\n",
    "\n",
    "### Linealidad en R\n",
    "\n",
    "La función de $R$ para representar estos gráficos es ``crPlots`` del paquete **car**: ``crPlots(r)``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observaciones anómalas.\n",
    "\n",
    "Las observaciones anómalas pueden provocar que se malinterpreten patrones en el conjunto de datos. Además, puntos aislados pueden tener una gran influencia en el modelo de regresión, dando resultados completamente diferentes. Por ejemplo, pueden provocar que nuestro modelo no capture características importantes de los datos. \n",
    "\n",
    "Por ello, es importante detectarlas.\n",
    "\n",
    "Existen tres tipos de observaciones anómalas:\n",
    "\n",
    "* **Outliers de regresión** son observaciones que tienen un valor anómalo de la variable $Y$, condicionado a los valores de sus variables independientes $X_i$. Tendrán un residuo muy alto pero no pueden afectar demasiado a los coeficientes de la regresión.\n",
    "\n",
    "\n",
    "* **Leverages.** son observaciones con un valor anómalo de las variables $X_i$. No tienen por qué afectar los coeficientes de la regresión.\n",
    "\n",
    "\n",
    "* **Observaciones influyentes** son aquellas que tienen un leverage alto; son outliers de regresión y afectan fuertemente a la regresión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverages\n",
    "\n",
    "Para hallar las observaciones que son leverages, necesitamos la matriz Hat: $H=X(X^TX)^{-1}X^T$. Como $\\widehat{y}=Hy$, tenemos que $\\widehat{y_j}=h_{1j}y_1+h_{2j}y_2+...+h_{nj}y_n=\\sum_{i=1}^nh_{ij}x_i$ para $j=1,...,n$.\n",
    "\n",
    "Si $h_{ij}$ es grande, la observación $i$-ésima tiene un impacto sustancial en el valor predicho $j$-ésimo.\n",
    "\n",
    "Se define el leverage de la observación $i$ como su valor hat: $h_i=\\sum_{j=1}^nh_{ij}^2$.\n",
    "hat\n",
    "La regla de decisión que nos dirá si una observación tiene leverage grande (y por lo tanto debe ser considerada con cuidado) es cuando su valor hat es mayor que $2(k+1)/n$.\n",
    "\n",
    "La función ``hatvalues`` de R calcula los valores hat basándose en la regla anterior: ``which(hatvalues(lm1)>2(k+1)/n)``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Para detectar outliers, en R se usa la ``función outlierTest`` del paquete **car**:  ``outlierTest(lm1)``. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones influyentes.\n",
    "\n",
    "El método que usaremos es la llamada Distanca de Cook. Las observaciones influyentes son aquellas cuya distancia de Cook es mayor a $4/(n-k-1)$\n",
    "\n",
    "En R, la distancia de Cook se calcula con ``cooks.distance`` del paquete **car**: ``which(cooks.distance(lm1)>4/(n-k-1))``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamiento de las observaciones anómalas.\n",
    "\n",
    "El tratamiento suele ser complejo. \n",
    "\n",
    "Nos debemos preguntar si se deben a errores en la entrada o recogida de los datos y si este es el caso, eliminarlas.\n",
    "\n",
    "Pero también pueden explicar que no se ha considerado alguna variable independiente que afecta al conjunto de observaciones anómalas.\n",
    "\n",
    "Las más peligrosas son las influyentes.\n",
    "\n",
    "En el supuesto de que se determine que se pueden eliminar, se tienen que eliminar de una a una actualizando el modelo cada vez. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
