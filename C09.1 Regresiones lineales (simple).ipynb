{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las regresiones son maneras de \\emph{modelar} una característica medida utilizando otras, también medidas, de la misma muestra con el objetivo de crear predicciones. Esto es: si $X_1,X_2,...,X_n,X_{n+1}$ son algunas de las columnas de la tabla, encontrar una función $f$ tal que $X_{n+1}=f(X_1,X_2,...,X_n)$. En cristiano: **¿será posible explicar el comportamiento de una de las características a través del conocimiento de otras?**\n",
    "\n",
    "Bajo la idea anterior, decimos que las características $X_1,X_2,...,X_n$ son **explicativas** o **predictoras** y la característica $X_{n+1}$ es la **variable objetivo** o **a predecir**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión lineal simple\n",
    "\n",
    "En esta sección platicaremos de un problema de regresión muy sencillo conocido como **regresión lineal**. Observemos la siguiente nube de puntos: \n",
    "\n",
    "![imagenes](im016.png)\n",
    "\n",
    "Debido a su forma, vale preguntarse cuál será la recta que mejor se aproxime, en algún sentido, a todos los puntos al mismo tiempo.\n",
    "\n",
    "Observemos varias rectas graficadas con la nube de puntos. ¿Cuál dirías que es la que más se *ajusta* a todos los puntos al mismo tiempo?\n",
    "\n",
    "![imagenes](im017.png)\n",
    "\n",
    "De esta manera, sean $X$ y $Y$ dos características de tu población. Decimos que el modelo que explica a $Y$ a través de $X$ es lineal si tenemos razones para pensar que existen números $\\beta_0$ y $\\beta_1$ tales que $Y=\\beta_0+\\beta_1X+\\varepsilon$ donde $\\varepsilon$ es una variable aleatoria gaussiana con media 0 (un ruido blanco)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometría analítica\n",
    "\n",
    "Antes de avanzar, recordemos nuestros cursos de Geometría Analítica: toda recta en el plano es una ecuación de la forma $$y=mx+b$$ donde \n",
    "\n",
    "* $b$ se conoce como *ordenada al origen* y es el valor sobre el eje Y en que en la recta lo atraviesa. En regresión lineal se le llama *intercepto*.\n",
    "\n",
    "* $m$ se conoce como *pendiente de la recta* y se identifica como la tangente inversa del ángulo que hace la recta con el eje X. En cristiano: $m$ mide la inclinación de la recta. También en regresión lineal se llama pendiente. Si $m>0$ la recta va hacia arriba; si $m<0$ la recta va hacia abajo; si $m=0$, la recta es horizontal.\n",
    "\n",
    "Observa la interacción del archivo **geogebra-recta_pendiente_origen.ggb** de nuestro repositoria [que mostramos aquí con GeoGebra](https://github.com/scidatmath2020/Inferencia-Estad-stica-2022/blob/main/geogebra-recta_pendiente_origen.ggb)\n",
    "\n",
    "Por lo tanto, **hallar la ecuación de una recta equivale a hallar los valores de $m$ y $b$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mínimos cuadrados\n",
    "\n",
    "\n",
    "\n",
    "Recordemos nuestro objetivo: tenemos una lista de parejas de puntos $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$. Los graficamos como nube de puntos y buscamos la recta que mejor aproxime, en cierto sentido, a todos los puntos al mismo tiempo. \n",
    "\n",
    "En principio, cualquier recta $y=mx+b$ es una recta que podemos tomar como aproximación. Por lo tanto, a cada $x_i$ se le asignan dos números: el $y_i$ (que es un valor que conocemos) y el $\\hat{y_i}=mx_i+b$, que es el valor que nos da la recta para ese número $x_i$.\n",
    "\n",
    "* **Predicciones.** Son los valores $\\hat{y_1},\\hat{y_2},...,\\hat{y_n}$. Es decir, los valores que *la recta predice*.\n",
    "\n",
    "* **Residuos.** ¿Qué tanto se equivocó la recta? Recordemos: la recta le asigna a $x_i$ el valor $\\hat{y_i}$. Pero el valor verdadero que acompaña a $x_i$ es $y_i$. Los residuos son los errores que la recta cometió: $\\varepsilon_i=y_i-\\hat{y_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dicho esto, lo que buscamos es minimizar los residuos al mismo tiempo. Esto es: hallar $m$ y $b$ tales que $$\\sum_{i=1}^n(y_i-\\hat{y_i})^2$$ es lo menor posible.\n",
    "\n",
    "Es decir, dadas tus observaciones $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$ hallar $m$ y $b$ tales $$\\sum_{i=1}^n(y_i-mx_i-b)^2$$ sea mínima. \n",
    "\n",
    "Esto es lo que se conoce como **problema de los mínimos cuadrados.**\n",
    "\n",
    "Se puede demostrar fácilmente que el $m$ y $b$ que resuelven este problema son $$m=\\frac{\\sum(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum(x_i-\\overline{x})^2}\\mbox{ y }b=\\overline{y}-m\\overline{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto tenemos el siguiente resultado:\n",
    "\n",
    "Si $(X_1,Y_1),(X_2,Y_2),...,(X_n,Y_n)$ son una muestra de tamaño $n$ de la característica $(X,Y)$ (es decir, $n$ renglones de tu tabla tomando solo las columnas $X$ y $Y$), entonces los estimadores para $\\beta_0$ y $\\beta_1$ de la recta de mínimos cuadrados son $$b_1=\\frac{\\sum(X_i-\\overline{X})(Y_i-\\overline{Y})}{\\sum(X_i-\\overline{X})^2}\\,\\,\\mbox{ y \n",
    "}\\,\\,b_0=\\overline{Y}-b_1\\overline{X}$$\n",
    "\n",
    "***\n",
    "***\n",
    "**Ejemplo 1. Datos**\n",
    "\n",
    "En un experimento donde se quería estudiar la asociación entre consumo de sal y presión arterial, se asignó aleatoriamente a algunos individuos una cantidad diaria constante de sal en su dieta, y al cabo de un mes se los midió la tensión arterial media. Algunos resultados fueron los siguientes:\n",
    "\n",
    "$X$(sal, en g)|$Y$(Presión, en mm de Hg)\n",
    ":--:|:--:\n",
    "1.8\t|100\n",
    "2.2\t|98\n",
    "3.5\t|110\n",
    "4.0\t|110\n",
    "4.3\t|112\n",
    "5.0\t|120\n",
    "\n",
    "En **R** esto es\n",
    "\n",
    "``presion <- data.frame(sal=c(1.8,2.2,3.5,4.0,4.3,5.0),tension=c(100,98,110,110,112,120))``\n",
    "\n",
    "Observemos la siguiente gráfica de los datos:\n",
    "\n",
    "![imagenes](im018.png)\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal simple en R\n",
    "\n",
    "En **R**, existe la función ``lm(\"nombre de la variable a predecir\"~\"nombre de la variable predictora\",data=tabla)`` para realizar regresiones lineales. \n",
    "\n",
    "Haciendo ``summary(resultados_regresion_lineal)`` veremos un resumen general de los resultados obtenidos. Los coeficientes $b_0$ y $b_1$ se obtienen, en ese orden, mediante ``coefficients(resultados_regresion_lineal)``\n",
    "\n",
    "***\n",
    "***\n",
    "**Ejemplo 1. Coeficientes**\n",
    "\n",
    "De esta manera, en el **Ejemplo 1** tenemos:\n",
    "\n",
    "``resultados_regresion_lineal  <- lm(tension~sal,data=presion)\n",
    "coefficients(resultados_regresion_lineal)``\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efectividad de la recta de regresión\n",
    "\n",
    "En general, dadas las características $X$ y $Y$ y al menos dos observaciones (es decir, dos renglones de la tabla), **siempre será posible hallar la recta de regresión lineal**. Sin embargo, esto no nos dice si la recta es un buen modelo o no. Es decir, si la inferencia que hagamos a partir de ella será confiable o no.\n",
    "\n",
    "Una forma de verificar si nuestras características $X$ y $Y$ obedecen a un modelo lineal es el **coeficiente de determinación $R^2$**\n",
    "\n",
    "Este se define a través de la **identidad de suma de cuadrados** (muy parecida a la del ANOVA):\n",
    "\n",
    "$$\\underbrace{\\sum_{i=1}^n(Y_i-\\overline{Y})^2}_{SS_{Total}}=\\underbrace{\\sum_{i=1}^n(\\hat{Y_i}-\\overline{Y})^2}_{SS_{Regresión}}+\\underbrace{\\sum_{i=1}^n(Y_i-\\hat{Y_i})^2}_{SS_{Error}}$$\n",
    "\n",
    "Como hemos dicho, la filosofía es que $SS_{Error}$ sea lo mínimo posible y, si es chica entonces la variación total de los datos originales viene fuertemente explicada por la variación de los datos de nuestas predicciones.\n",
    "\n",
    "Se define el **coeficiente de determinación** como $$R^2=\\frac{SS_{Regresión}}{SS_{Total}}$$\n",
    "\n",
    "Cuanto más cerca esté el $R^2$ a 1, es mejor nuestra regresión. Y si $R^2$ es próximo a 0, entonces no existe modelo lineal que relacione a $X$ con $Y$ y por lo tanto la recta de regresión lineal no nos sirve para nada.\n",
    "\n",
    "## R^2 en R\n",
    "\n",
    "En **R**, simplemente hacemos ``summary(resultados_regresion_lineal)$r.squared`` para obtener el coeficiente de determinación.\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "**Ejemplo 1. Coeficiente de determinación**\n",
    "\n",
    "``summary(resultados_regresion_lineal)$r.squared``\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervalos de confianza de los coeficientes en la regresión lineal simple\n",
    "\n",
    "Recordemos que $b_0$ y $b_1$ dependen de la muestra, por lo que a su vez son variables aleatorias que nos siven para estimar $\\beta_0$ y $\\beta_1$. Bajo ciertos supuestos que veremos en la siguiente sección (Regresión múltiple), podemos encontrar intervalos de confianza para $\\beta_0$ y $\\beta_1$:\n",
    "\n",
    "$$\\beta_0:\\,\\,\\,\\left(b_0-t_{n-2,1-\\alpha/2}S\\sqrt{\\frac{1}{n}+\\frac{\\overline{X}^2}{(n-1)S_X ^2}},b_0+t_{n-2,1-\\alpha/2}S\\sqrt{\\frac{1}{n}+\\frac{\\overline{X}^2}{(n-1)S_X ^2}}\\right)$$\n",
    "\n",
    "$$\\beta_1:\\,\\,\\,\\left(b_1-t_{n-2,1-\\alpha/2}\\frac{S}{S_X\\sqrt{n-1}},b_1-t_{n-2,1+\\alpha/2}\\frac{S}{S_X\\sqrt{n-1}}\\right)$$\n",
    "\n",
    "donde $S$ es la desviación muestral de los errores y $S_X$ es la desviación muestral de la variable $X$.\n",
    "\n",
    "## Intervalos de confianza de los coeficientes en R\n",
    "\n",
    "En **R** los intervalos de confianza al $100*(1-\\alpha)$% vienen dados por ``confint(resultados_regresion_lineal,level=1-alpha)``\n",
    "\n",
    "***\n",
    "***\n",
    "**Ejemplo 1. Intervalos de confianza de los coeficientes**\n",
    "\n",
    "``confint(resultados_regresion_lineal,level=0.95)``\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significancia de la pendiente $\\beta_1$\n",
    "\n",
    "Cuando decimos que $Y$ se explica linealmente por $X$, implícitamente estamos diciendo que en el modelo $$Y=\\beta_0+\\beta_1X+\\varepsilon$$ tenemos $\\beta_1\\neq0$ (ya que si $\\beta_1=0$ entonces $Y$ no dependería de $X$ y por lo tanto la variación de $Y$ únicamente depende de $\\varepsilon$ que es totalmente azar).\n",
    "\n",
    "Por lo tanto se plante el contraste $$\\begin{array}{l}H_0:\\beta_1=0\\\\H_1:\\beta_1\\neq0\\end{array}$$\n",
    "\n",
    "En este caso, una vez calculado el $p$-valor correspondiente, tenemos la regla de decisión:\n",
    "\n",
    "$p$-valor|Descripción|Significado\n",
    ":--:|:--|:--\n",
    "$<0.05$|Tenemos evidencia suficiente para rechazar $H_0$|Sí hay efectos de $X$ sobre $Y$\n",
    "$>0.1$|No tenemos evidencia suficiente para rechazar $H_0$|No hay efectos de $X$ sobre $Y$\n",
    "en otro caso||Necesitamos mas información|\n",
    "\n",
    "El $p$-valor viene dado en ``summary(resultados_regresion_lineal)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones en la regresión lineal simple\n",
    "\n",
    "Nuevamente, bajo ciertos supuestos que veremos en la siguiente sección (Regresión múltiple), la utilidad de las regresiones lineales radica en que nos sirven para realizar predicciones. Es decir, si llega un nuevo dato de la variable $X$, digamos $x_0$, podemos utilizar el modelo calculado a partir de la tabla original para encontrar el valor de $Y$ en $x_0$: $$Y|_{x_0}=b_0+b_1x_0$$\n",
    "\n",
    "En otras palabras, nos puede interesar el valor medio de la variable de respuesta cuando un valor $x_0$ no ha sido considerado en la muestra, o el valor de la variable respuesta también cuando un valor $x_0$ no ha sido considerado en la muestra.\n",
    "\n",
    "Para ambos valores usamos como estimación a $b_0+b_1x_0$.\n",
    "\n",
    "## Predicciones en la regresión lineal simple en R\n",
    "\n",
    "Para hallar estos intervalos de confianza cuando $X=x_0$ al $100(1-\\alpha)$% de confianza debemos usar la función ``predict.lm`` de la siguiente manera:\n",
    "\n",
    "``new.data = data.frame(x=x0)\n",
    "predict.lm(resultados_regresion_lineal,newdata,interval=\"confidence\",level=1-alpha)\n",
    "predict.lm(resultados_regresion_lineal,newdata,interval=\"prediction\",level=1-alpha)`` \n",
    "\n",
    "***\n",
    "***\n",
    "**Ejemplo 1. Predicciones**\n",
    "\n",
    "Suponiendo que a un paciente se le administra una dosis de sal de 4.5, ¿cuál va a ser su presión promedio y el intervalo de confianza de su presión promedio al 95%? ¿Cuál va a ser su presión y el intervalo de confianza para su presión?\n",
    "\n",
    "``new.data = data.frame(sal=4.5)\n",
    "predict.lm(resultados_regresion_lineal,newdata,interval=\"confidence\",level=0.95)\n",
    "predict.lm(resultados_regresion_lineal,newdata,interval=\"prediction\",level=0.95)``\n",
    "\n",
    "\n",
    "***\n",
    "***\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
